# lightning.pytorch==2.1.3
seed_everything: false
eval_after_fit: true
trainer:
  accelerator: gpu
  devices: 1
  precision: 16-mixed
  max_epochs: 200
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: logs/wideresnet28x10
      name: deep_ensembles
      default_hp_metric: false
  callbacks:
    - class_path: torch_uncertainty.callbacks.TUClsCheckpoint
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val/cls/Acc
        patience: 1000
        check_finite: true
model:
  model:
    class_path: torch_uncertainty.models.deep_ensembles
    init_args:
      core_models:
        class_path: torch_uncertainty.models.classification.wideresnet28x10
        init_args:
          in_channels: 3
          num_classes: 10
          style: cifar
          dropout_rate: 0.3
      num_estimators: 4
      task: classification
      # eventually you can pass the checkpoints of standard wideresnet28x10 models here
      # ckpt_paths: [path/to/ckpt1, path/to/ckpt2, path/to/ckpt3, path/to/ckpt4]
  num_classes: 10
  loss: CrossEntropyLoss
  is_ensemble: true
  format_batch_fn:
    class_path: torch_uncertainty.transforms.RepeatTarget
    init_args:
      num_repeats: 4
data:
  root: ./data
  batch_size: 128
optimizer:
  class_path: torch.optim.SGD
  init_args:
    lr: 0.4  # initial learning rate times 4 (num_estimators)
    momentum: 0.9
    weight_decay: 5e-4
    nesterov: true
lr_scheduler:
  class_path: torch.optim.lr_scheduler.MultiStepLR
  init_args:
    milestones:
    - 60
    - 120
    - 160
    gamma: 0.2
